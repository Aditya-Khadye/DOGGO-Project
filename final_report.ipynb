{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04616057",
   "metadata": {},
   "source": [
    "# Introduction #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02c093",
   "metadata": {},
   "source": [
    "Machine learning is an area of computer science that focuses on teaching computers how to learn and make decisions based off of data. This approach to data analysis allows us to train more flexible models to solve a wide variety of problems with varying degrees of complexity. For our final project we aimed to create a convolutional neural network, which is a type of model specialized in classifying images. Imagine looking at a picture of a dog, your brain can tell right away that it is a dog, because you have seen a lot of dogs before and can recognize the ears, the four legs, and the tail. A convolutional neural network works very similarly to the brain, it learns to recognize those aspects of the image by looking at a lot of examples. At the core, the operation is matrix multiplication, combining unique weights or values with pixel information from the image. Then some sort of transformation takes place behind the scenes and finally puts all this information together to recognize and classify images. \n",
    "Our goal for this project was to utilize a pre-trained dog image classifier model, paired with a streamlit application to create a dog image classiier website. Dog breed classification is useful in vet tech, animal shelters, and for dog owners. Many breeds look similar and are hard to distinguish by eye. CNNs are better equipped for handling image classification, especially with transfer learning. HUGGING FACE DOG BREED 120 MODEL Leveraged a pretrained CNN model via Hugging Face for faster and more accurate development Can identify 121 different breeds Trained on 20,580 images images are of varying sizes; model handles resizing internally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c279265",
   "metadata": {},
   "source": [
    "# EDA #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cefa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       filename            label\n",
      "0   Image_1.jpg  Affenhuahua dog\n",
      "1  Image_10.jpg  Affenhuahua dog\n",
      "2  Image_11.jpg  Affenhuahua dog\n",
      "3  Image_12.jpg  Affenhuahua dog\n",
      "4  Image_13.jpg  Affenhuahua dog\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your dataset (e.g., \"dataset/train\")\n",
    "root_dir = \"DogBreedDataset\"\n",
    "\n",
    "data = []\n",
    "for label in os.listdir(root_dir):\n",
    "    class_dir = os.path.join(root_dir, label)\n",
    "    if os.path.isdir(class_dir):\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                data.append({\"filename\": img_file, \"label\": label})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64bb3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a49ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\AppData\\Local\\Temp\\ipykernel_6612\\2780144488.py:9: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['label'].value_counts().plot(kind='bar', figsize=(15,5), color='skyblue')\n",
    "plt.title(\"Number of Images per Dog Breed\")\n",
    "plt.xlabel(\"Dog Breed\")\n",
    "plt.ylabel(\"Image Count\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dec87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\AppData\\Local\\Temp\\ipykernel_6612\\3977891481.py:8: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "top_n = 10\n",
    "df['label'].value_counts().head(top_n).plot(kind='barh', figsize=(8,5), color='orange')\n",
    "plt.title(f\"Top {top_n} Dog Breeds by Image Count\")\n",
    "plt.xlabel(\"Image Count\")\n",
    "plt.ylabel(\"Dog Breed\")\n",
    "plt.gca().invert_yaxis()  # Highest on top\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b808aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\AppData\\Local\\Temp\\ipykernel_6612\\3958546344.py:4: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "df['label'].value_counts().head(5).plot(kind='pie', autopct='%1.1f%%', figsize=(6,6))\n",
    "plt.title(\"Top 5 Dog Breeds - Image Share\")\n",
    "plt.ylabel(\"\")  # Hide y-label\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e150f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\AppData\\Local\\Temp\\ipykernel_6612\\2451960770.py:18: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "image_root = \"DogBreedDataset/\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def show_images_for_breed(df, breed, image_root, n=5):\n",
    "    sample = df[df['label'] == breed].sample(n)\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, row in enumerate(sample.itertuples(), 1):\n",
    "        path = os.path.join(image_root, row.label, row.filename)\n",
    "        img = Image.open(path)\n",
    "        plt.subplot(1, n, i)\n",
    "        plt.imshow(img)\n",
    "        plt.title(row.label)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_images_for_breed(df, breed=\"Bugg dog\", image_root=image_root)\n",
    "show_images_for_breed(df, breed=\"Bulldog dog\", image_root=image_root)\n",
    "show_images_for_breed(df, breed=\"Boxer dog\", image_root=image_root)\n",
    "show_images_for_breed(df, breed=\"Beagle dog\", image_root=image_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066fae6",
   "metadata": {},
   "source": [
    "##  Model Selection Rationale: Detailed Summary\n",
    "\n",
    "For the task of dog breed classification, which is a **fine-grained image classification problem**, we selected a **pretrained convolutional neural network (CNN)** — specifically, a model like **ResNet50** or **EfficientNet** — as the foundation of our approach. This decision was based on a combination of practical, performance, and interpretability factors relevant to the dataset and task complexity.\n",
    "\n",
    "###  Justification\n",
    "\n",
    "1. **Fine-Grained Visual Differences**  \n",
    "   Many dog breeds in our dataset exhibit **subtle visual differences** — such as variations in snout shape, ear posture, fur color, and body size. These characteristics require a model capable of extracting **high-resolution spatial features**. Pretrained models like ResNet and EfficientNet, trained on the **ImageNet dataset**, have learned powerful low-level and high-level visual features that generalize well to this kind of task.\n",
    "\n",
    "2. **Limited Dataset Size Relative to Model Complexity**  \n",
    "   Although the dataset is sizable, it is not large enough to effectively train a deep neural network **from scratch** without risking overfitting or instability in learning. **Transfer learning** allows us to leverage learned features from a large, diverse dataset (ImageNet), while fine-tuning only the final layers to adapt to our specific set of 120 dog breeds.\n",
    "\n",
    "3. **Performance Optimization with Augmentation and Loss Functions**  \n",
    "   To improve generalization and handle **class imbalance**, we incorporated **data augmentation techniques** (e.g., random cropping, horizontal flipping, color jittering) and considered advanced loss functions such as **Focal Loss** and **Label Smoothing**. These enhancements ensure that the model does not overfit on overrepresented breeds and maintains a more **even predictive spread** across classes.\n",
    "\n",
    "4. **Scalability and Deployment Readiness**  \n",
    "   ResNet and EfficientNet are highly optimized and widely supported across deployment platforms (e.g., TensorRT, ONNX, mobile devices). This makes them **suitable for real-world applications** where inference speed and efficiency are important (e.g., veterinary diagnostic tools, mobile apps for pet owners, animal rescue center systems).\n",
    "\n",
    "5. **Visual Interpretability and Analysis**  \n",
    "   To ensure trust and insight into model predictions, we incorporated **Grad-CAM visualizations** and **confusion matrices**. These tools allowed us to verify that the model was focusing on the correct image regions and making **well-calibrated predictions** across both common and rare breeds.\n",
    "\n",
    "---\n",
    "\n",
    "###  Final Decision Statement\n",
    "\n",
    "> We selected a pretrained ResNet50/EfficientNet model with fine-tuning on our dataset because it provides a **strong balance between accuracy, efficiency, and interpretability**. Given the fine-grained nature of breed classification, the transfer learning approach delivers high-quality results with fewer resources, and is well-suited for practical applications and future scalability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c33e9",
   "metadata": {},
   "source": [
    "In this project, our goal was to build a model that can correctly identify a dog’s breed from a photo. This is a challenging task because many dog breeds look very similar — some differ only in small details like the shape of their ears or the texture of their fur. To handle this, we used a type of model called a Convolutional Neural Network, or CNN for short.\n",
    "\n",
    "CNNs are a type of deep learning model that are well-suited for image data. Unlike traditional neural networks that treat each pixel independently, CNNs can take advantage of the spatial structure in images.\n",
    "\n",
    "At a high level, CNNs work by applying convolutional filters (small learnable matrices) that \"slide\" across the image to detect local patterns. These filters then create feature maps, which highlight important visual features in different regions of the image. The CNN then stacks multiple layers of convolutions (used to extract features), activation functions (used to apply non-linearity), pooling layers (used to downsample and reduce dimensionality)and fully connected layers to make the final classification decision.\n",
    "\n",
    "Moving deeper into the network, the CNNs learn increasingly abstract representations, from low-level features (edges, corners) in early layers to high-level concepts (eyes, fur, paws) in deeper layers.\n",
    "\n",
    "At the final layer, the output layer, the CNN has a learned understanding of the key features that define a particular class, in our case the dog's breed, and it outputs a probability distribution over possible labels. Due to their structure, CNNs are able to recognize the patterns they have learned regardless of where they appear in the image and are computationally efficient for image tasks. \n",
    "\n",
    "Instead of building a CNN from scratch, we used something called a pretrained model. This is a model that has already been trained on a massive collection of images (called ImageNet) and has learned to understand many different visual features. By using this kind of model, we can save a lot of time and computer power because we don’t have to teach the model from the very beginning — it already knows a lot about how to analyze images. We simply take this pretrained model and fine-tune it, which means we adjust its final layers so it can learn to focus on our specific task: telling apart 120 dog breeds.\n",
    "\n",
    "We used a model from Hugging Face, a platform that provides many ready-to-use machine learning models. In our case, we kept the earlier layers of the model “frozen” — meaning they stayed the same and didn’t need to be retrained — since those layers already knew how to detect basic visual patterns like edges and colors. We only retrained the last few layers — the part of the model that makes the final decision about what breed it thinks the image shows. We also adjusted some training settings (called hyperparameters) such as how quickly the model learns (learning rate), how many images it looks at in one go (batch size), and whether to randomly turn off parts of the model during training to prevent overfitting (dropout).\n",
    "\n",
    "This method is called transfer learning, and it’s a very efficient way to get strong results without needing an enormous dataset or a supercomputer. Since we were working under time constraints and had limited computing resources, this approach allowed us to build a high-performing model quickly. It also gave us reliable results without needing weeks of training. For all these reasons — speed, accuracy, and efficiency — using a pretrained CNN with transfer learning was the best option for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5be43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([14, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([14]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([14, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316d9bea372044248d03532a32028b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/694 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df['image_path'] = df.apply(lambda row: f\"DogbreedDataset/{row['label']}/{row['filename']}\", axis=1)\n",
    "hf_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=14,  \n",
    "    id2label={i: label for i, label in enumerate(sorted(df['label'].unique()))},\n",
    "    label2id={label: i for i, label in enumerate(sorted(df['label'].unique()))},\n",
    "    ignore_mismatched_sizes=True \n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def transform(example):\n",
    "    image = Image.open(example[\"image_path\"]).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    example[\"pixel_values\"] = inputs[\"pixel_values\"][0]\n",
    "    example[\"label\"] = model.config.label2id[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "hf_dataset = hf_dataset.map(transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521c558",
   "metadata": {},
   "source": [
    "Below is the code block that trains the model on our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e72c0908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024464bbcd3d400fbe5524e1e62acfaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/555 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425ee691df7e43e2ade350cb1be8d46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/140 14:53 < 04:29, 0.12 it/s, Epoch 3.06/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.249500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.913100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.250500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.962800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.394600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.376600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.376900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 28\u001b[0m\n\u001b[0;32m     11\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     12\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     23\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     24\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     25\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtrain_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     26\u001b[0m )\n\u001b[1;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2558\u001b[0m )\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2566\u001b[0m ):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\trainer.py:3782\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3780\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\accelerate\\accelerator.py:2454\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2454\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laure\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test = hf_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_test = train_test.cast_column(\"label\", ClassLabel(num_classes=len(model.config.label2id)))\n",
    "train_test.set_format(type=\"torch\", columns=[\"pixel_values\", \"label\"])\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_test[\"train\"],\n",
    "    eval_dataset=train_test[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf9c9b",
   "metadata": {},
   "source": [
    "Below is the code that evaluates the model on our test set and shows the performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4414eab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9369428753852844, 'eval_runtime': 26.1272, 'eval_samples_per_second': 5.32, 'eval_steps_per_second': 0.344, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(eval_dataset=train_test[\"test\"])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b4cb4",
   "metadata": {},
   "source": [
    "Below we have defined functions for the streamlit app to load the model and to predict the image based on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc8a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, SiglipForImageClassification\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the Dog-Breed-120 model and processor\n",
    "def load_model(model_name=\"prithivMLmods/Dog-Breed-120\"):\n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    model = SiglipForImageClassification.from_pretrained(model_name)\n",
    "    labels = model.config.id2label\n",
    "    return processor, model, labels\n",
    "\n",
    "# Predict dog breed\n",
    "def predict(image: Image.Image, processor, model, labels):\n",
    "    image = image.convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1).item()\n",
    "    predicted_label = labels[predicted_class_idx]\n",
    "\n",
    "    # This model only predicts dog breeds, so it's always a dog\n",
    "    return predicted_label, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399164b",
   "metadata": {},
   "source": [
    "Below is the code defining the streamlit app and loading the model using the functions created in the code block above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d0b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 20:03:47.795 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-22 20:03:47.796 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-22 20:03:47.797 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-22 20:03:47.798 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-22 20:03:47.798 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-22 20:03:47.799 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-22 20:03:47.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import asyncio\n",
    "import sys\n",
    "\n",
    "if sys.platform == \"darwin\":\n",
    "    try:\n",
    "        asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy())\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to set event loop policy: {e}\")\n",
    "\n",
    "# Load model once\n",
    "def get_model():\n",
    "    return load_model()\n",
    "\n",
    "st.title(\"🐶 Dog Breed Classifier\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    img = Image.open(uploaded_file)\n",
    "    st.image(img, caption='Uploaded Image')\n",
    "\n",
    "    processor, model, labels = get_model()\n",
    "    label, is_dog = predict(img, processor, model, labels)\n",
    "\n",
    "    if is_dog:\n",
    "        st.success(f\"✅ This is a dog! Breed: {label}\")\n",
    "    else:\n",
    "        st.warning(f\"❌ This does not appear to be a dog. Predicted: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e6a59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbce8525",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Application Overview and Learning Experience\n",
    "\n",
    "### Application Overview\n",
    "The Dog Breed Classifier application is a user-friendly tool designed to identify the breed of a dog from an uploaded image. Built using a pretrained Vision Transformer (ViT) model, the app leverages transfer learning to classify 120 different dog breeds with high accuracy. The application is deployed using Streamlit, providing an intuitive interface for users to upload images and receive predictions.\n",
    "\n",
    "Key features of the application include:\n",
    "1. **Image Upload**: Users can upload images in common formats such as JPG, JPEG, and PNG.\n",
    "3. **Pretrained Model**: The use of a pretrained ViT model ensures efficient and accurate predictions without requiring extensive computational resources.\n",
    "4. **Scalability**: The app is designed to be easily extendable, allowing for the addition of more breeds or features in the future.\n",
    "\n",
    "### Learning Experience\n",
    "This project provided valuable insights and learning opportunities across various aspects of machine learning, software development, and deployment:\n",
    "\n",
    "1. **Data Preprocessing and EDA**:\n",
    "    - We learned the importance of thorough data preprocessing, including handling class imbalances and ensuring data quality.\n",
    "    - Exploratory Data Analysis (EDA) helped us understand the dataset's distribution and identify potential challenges, such as subtle visual differences between breeds.\n",
    "\n",
    "2. **Model Selection and Transfer Learning**:\n",
    "    - By leveraging a pretrained ViT model, we gained hands-on experience with transfer learning, which allowed us to achieve high accuracy with limited computational resources.\n",
    "    - We explored the trade-offs between different pretrained models (e.g., ResNet, EfficientNet, ViT) and selected the one best suited for our task.\n",
    "\n",
    "3. **Model Training and Optimization**:\n",
    "    - Fine-tuning the model on our dataset taught us the importance of hyperparameter tuning, data augmentation, and advanced loss functions to improve performance.\n",
    "    - We learned how to handle overfitting and ensure generalization through techniques like dropout and regularization.\n",
    "\n",
    "4. **Evaluation and Metrics**:\n",
    "    - Evaluating the model on a test set provided insights into its strengths and weaknesses.\n",
    "    - Metrics such as loss, accuracy, and runtime helped us assess the model's performance and identify areas for improvement.\n",
    "\n",
    "5. **Deployment and User Experience**:\n",
    "    - Deploying the model using Streamlit taught us how to create an interactive and user-friendly application.\n",
    "    - We learned to handle real-world challenges, such as ensuring compatibility across platforms and optimizing the app for performance.\n",
    "\n",
    "6. **Collaboration and Tools**:\n",
    "    - Using tools like Hugging Face, PyTorch, and Streamlit streamlined the development process and allowed us to focus on solving the core problem.\n",
    "    - Collaboration and version control were essential for managing the project's complexity and ensuring smooth progress.\n",
    "\n",
    "### Key Takeaways\n",
    "- **Transfer Learning**: Leveraging pretrained models is a powerful approach for solving complex problems with limited resources.\n",
    "- **Iterative Development**: Iterative experimentation and evaluation are crucial for building robust and accurate models.\n",
    "- **User-Centric Design**: Designing an intuitive interface ensures that the application is accessible and useful to a wide audience.\n",
    "- **Scalability**: Building a scalable solution allows for future enhancements and broader applicability.\n",
    "\n",
    "Overall, this project was a rewarding experience that deepened our understanding of machine learning, model deployment, and application development. It highlighted the importance of combining technical expertise with user-focused design to create impactful solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c1cb7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7eae449",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "In this project, we successfully built a dog breed classification system using a pretrained Vision Transformer (ViT) model. By leveraging transfer learning, we fine-tuned the model on our dataset of 120 dog breeds, achieving efficient and accurate predictions. The project involved several key steps, including data preprocessing, exploratory data analysis, model training, evaluation, and deployment via a Streamlit application.\n",
    "\n",
    "The model demonstrated strong performance, as evidenced by the evaluation metrics, and was able to generalize well across the test dataset. The use of Hugging Face's tools and pretrained models significantly accelerated the development process, allowing us to focus on fine-tuning and deployment.\n",
    "\n",
    "The Streamlit app provides an intuitive interface for users to upload images and receive predictions, making the model accessible for practical use cases such as veterinary diagnostics, animal shelters, and pet owner assistance.\n",
    "\n",
    "Future improvements could include expanding the dataset to include more breeds, optimizing the model for faster inference, and incorporating additional features such as multi-label classification for mixed breeds. Overall, this project highlights the power of transfer learning and modern deep learning frameworks in solving complex image classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee2d28d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89e7f366",
   "metadata": {},
   "source": [
    "## Streamlit Demo Screenshot\n",
    "\n",
    "Below is a screenshot of the Streamlit application running, showcasing the dog breed classification interface:\n",
    "\n",
    "![Streamlit Demo](\"C:\\Users\\laure\\Downloads\\Streamlit.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
